{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraies\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n",
    "folder_path = '/Users/jasper/Desktop/LangChain/'\n",
    "\n",
    "# Folder path\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
    "  openai.api_key = ' '.join(f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the OpenAI API key by updating the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI class\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.9\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Maharaja's Palace\" or \"Spice Kingdom\"\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "# response = llm.predict(\"What is the capital of the United States?\")\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "response = llm.predict(\"What is the capital of the United States?\")\n",
    "#name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Sahafti Al-Filasteeni\" (My Palestinian Kitchen)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Palestinian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PromptTemplate class from the prompts module of the langchain package.\n",
    "# This class is  used to create, manipulate, or interact with prompts with the langchain framework.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the format method of the PromptTemplate object to replace the placeholder in the template with a specific value.\n",
    "# In this case, the 'cuisine' placeholder in the template is replaced with the string \"Italian\".\n",
    "# The resulting formatted prompt is assigned to the variable 'p'.\n",
    "# So, 'p' will contain the string \"I want to open a restaurant for Italian food. Suggest a fancy name for this.\"\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "# print the prompt\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Casa di Gastronomia\"\n"
     ]
    }
   ],
   "source": [
    "# Use the predict method of the OpenAI model object (llm) to generate a prediction based on the input prompt 'p'.\n",
    "# The input prompt 'p' is a string that was formatted using a PromptTemplate object.\n",
    "# The predict method returns a string that is the model's response to the input prompt.\n",
    "# The print function is used to display this response in the console.\n",
    "\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLMChain class from the chains module of the langchain package.\n",
    "# This class is used to create, manipulate, or interact with language model chains in the context of the langchain framework.\n",
    "\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The resulting LLMChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Mexican', 'text': '\\n\"El Sabroso Restaurante\" '}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the invoke method of the LLMChain object to generate a prediction based on the input string \"Mexican\".\n",
    "# The invoke method formats the input string using the chain's prompt template, passes the formatted prompt to the chain's language model, and returns the model's response.\n",
    "# In this case, the input string \"Mexican\" is likely used to replace the 'cuisine' placeholder in the prompt template.\n",
    "# So, the model's response will be a suggestion for a restaurant name for Mexican cuisine.\n",
    "\n",
    "#chain.run(\"Mexican\") -> run command is depreciated in the new version of langchain\n",
    "chain.invoke(\"Mexican\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "Prompt after formatting:\n",
    "I want to open a restaurant for Mexican food. Suggest a fency name for this.\n",
    "\n",
    "> Finished chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.6\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with the specified OpenAI model and prompt template.\n",
    "# The resulting LLMChain object is assigned to the variable 'name_chain'.\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'restaurant_name' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"Suggest some menu items for {restaurant_name}\"\n",
    "# So, when this template is used with a specific 'restaurant_name', it will generate a prompt for suggesting menu items for that restaurant.\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['restaurant_name'], template='Suggest some menu items for {restaurant_name}') llm=OpenAI(client=<class 'openai.api_resources.completion.Completion'>, temperature=0.6, openai_api_key='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', openai_proxy='')\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# In this case, the prompt template is designed to generate prompts for suggesting menu items for a specific restaurant.\n",
    "# The resulting LLMChain object is assigned to the variable 'food_items_chain'.\n",
    "# This 'food_items_chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)\n",
    "print(food_items_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SimpleSequentialChain class from the chains module of the langchain package.\n",
    "# This class is likely used to create, manipulate, or interact with simple sequential chains in the context of the langchain package.\n",
    "# A sequential chain is a type of model that processes input data in a sequential manner, which is often used in natural language processing.\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a SimpleSequentialChain object with specified parameters.\n",
    "# The chains parameter is a list of LLMChain objects that will be used by the SimpleSequentialChain in the order they are listed.\n",
    "# In this case, the list contains 'name_chain' and 'food_items_chain', which means 'name_chain' will be run first, followed by 'food_items_chain'.\n",
    "# The resulting SimpleSequentialChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified sequence of chains.\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Butter Chicken: A popular dish made with tender chicken pieces cooked in a rich and creamy tomato-based sauce.\n",
      "\n",
      "2. Biryani: Fragrant basmati rice cooked with choice of meat or vegetables, spices, and herbs.\n",
      "\n",
      "3. Tandoori Chicken: Marinated chicken cooked in a clay oven, resulting in a smoky and flavorful dish.\n",
      "\n",
      "4. Palak Paneer: A vegetarian dish made with cottage cheese cubes cooked in a creamy spinach gravy.\n",
      "\n",
      "5. Garlic Naan: Soft and fluffy Indian flatbread topped with garlic and served warm.\n",
      "\n",
      "6. Samosas: Crispy fried pastries filled with potatoes, peas, and spices, served with chutney for dipping.\n",
      "\n",
      "7. Rogan Josh: A Kashmiri lamb dish cooked in a spicy gravy with a blend of aromatic spices.\n",
      "\n",
      "8. Malai Kofta: Vegetarian dumplings made with paneer and vegetables, served in a creamy tomato-based sauce.\n",
      "\n",
      "9. Mango Lassi: A refreshing yogurt-based drink blended with mangoes and spices.\n",
      "\n",
      "10. Gulab Jamun: A popular Indian dessert made with fried dough balls soaked in a sweet syrup.\n"
     ]
    }
   ],
   "source": [
    "# Use the run method of the SimpleSequentialChain object to generate a prediction based on the input string \"Indian\".\n",
    "# The run method formats the input string using each chain's prompt template in the order they are listed, passes the formatted prompts to the chains' language models, and returns the models' responses in the same order.\n",
    "# In this case, the input string \"Indian\" is likely used to replace the 'cuisine' placeholder in the first chain's prompt template and the 'restaurant_name' placeholder in the second chain's prompt template.\n",
    "# The resulting responses will be a suggestion for a restaurant name for Indian cuisine and some menu items for that restaurant, respectively.\n",
    "\n",
    "content = chain.run(\"Indian\")\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.7\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
