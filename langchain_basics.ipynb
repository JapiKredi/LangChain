{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraies\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n",
    "folder_path = '/Users/jasper/Desktop/LangChain/'\n",
    "\n",
    "# Folder path\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
    "  openai.api_key = ' '.join(f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the OpenAI API key by updating the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI class\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.OpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.9\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Maharaja's Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "# response = llm.predict(\"What is the capital of the United States?\")\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "response = llm.predict(\"What is the capital of the United States?\")\n",
    "#name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. \"Olive Branch Bistro\"\\n2. \"Taste of Palestine\"\\n3. \"Zaytouna Kitchen\"\\n4. \"Falafel Kingdom\"\\n5. \"Hummus Haven\"\\n6. \"Mediterranean Delights\"\\n7. \"Heritage Kitchen\"\\n8. \"Saffron Oasis\"\\n9. \"Levant Cuisine\"\\n10. \"Sumaq Palace\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Palestinian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PromptTemplate class from the prompts module of the langchain package.\n",
    "# This class is  used to create, manipulate, or interact with prompts with the langchain framework.\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "# Use the format method of the PromptTemplate object to replace the placeholder in the template with a specific value.\n",
    "# In this case, the 'cuisine' placeholder in the template is replaced with the string \"Italian\".\n",
    "# The resulting formatted prompt is assigned to the variable 'p'.\n",
    "# So, 'p' will contain the string \"I want to open a restaurant for Italian food. Suggest a fancy name for this.\"\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")\n",
    "\n",
    "# print the prompt\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "La Dolce Vita Trattoria\n"
     ]
    }
   ],
   "source": [
    "# Use the predict method of the OpenAI model object (llm) to generate a prediction based on the input prompt 'p'.\n",
    "# The input prompt 'p' is a string that was formatted using a PromptTemplate object.\n",
    "# The predict method returns a string that is the model's response to the input prompt.\n",
    "# The print function is used to display this response in the console.\n",
    "\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Mexican', 'text': '\\n\\n\"Delicioso Cantina\"'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the LLMChain class from the chains module of the langchain package.\n",
    "# This class is used to create, manipulate, or interact with language model chains in the context of the langchain framework.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The resulting LLMChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# Use the invoke method of the LLMChain object to generate a prediction based on the input string \"Mexican\".\n",
    "# The invoke method formats the input string using the chain's prompt template, passes the formatted prompt to the chain's language model, and returns the model's response.\n",
    "# In this case, the input string \"Mexican\" is likely used to replace the 'cuisine' placeholder in the prompt template.\n",
    "# So, the model's response will be a suggestion for a restaurant name for Mexican cuisine.\n",
    "\n",
    "#chain.run(\"Mexican\") -> run command is depreciated in the new version of langchain\n",
    "chain.invoke(\"Mexican\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "Prompt after formatting:\n",
    "I want to open a restaurant for Mexican food. Suggest a fency name for this.\n",
    "\n",
    "> Finished chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.6\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "# Instantiate an LLMChain object with the specified OpenAI model and prompt template.\n",
    "# The resulting LLMChain object is assigned to the variable 'name_chain'.\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'restaurant_name' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"Suggest some menu items for {restaurant_name}\"\n",
    "# So, when this template is used with a specific 'restaurant_name', it will generate a prompt for suggesting menu items for that restaurant.\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# In this case, the prompt template is designed to generate prompts for suggesting menu items for a specific restaurant.\n",
    "# The resulting LLMChain object is assigned to the variable 'food_items_chain'.\n",
    "# This 'food_items_chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SimpleSequentialChain class from the chains module of the langchain package.\n",
    "# This class is likely used to create, manipulate, or interact with simple sequential chains in the context of the langchain package.\n",
    "# A sequential chain is a type of model that processes input data in a sequential manner, which is often used in natural language processing.\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleSequentialChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate a SimpleSequentialChain object with specified parameters.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The chains parameter is a list of LLMChain objects that will be used by the SimpleSequentialChain in the order they are listed.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# In this case, the list contains 'name_chain' and 'food_items_chain', which means 'name_chain' will be run first, followed by 'food_items_chain'.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# The resulting SimpleSequentialChain object is assigned to the variable 'chain'.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# This 'chain' object can be used to generate predictions based on the specified sequence of chains.\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleSequentialChain\u001b[49m(chains \u001b[38;5;241m=\u001b[39m [name_chain, food_items_chain])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleSequentialChain' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate a SimpleSequentialChain object with specified parameters.\n",
    "# The chains parameter is a list of LLMChain objects that will be used by the SimpleSequentialChain in the order they are listed.\n",
    "# In this case, the list contains 'name_chain' and 'food_items_chain', which means 'name_chain' will be run first, followed by 'food_items_chain'.\n",
    "# The resulting SimpleSequentialChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified sequence of chains.\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Indian', 'text': '\\n\\n\"Masala Majesty\"'}\n"
     ]
    }
   ],
   "source": [
    "# Use the run method of the SimpleSequentialChain object to generate a prediction based on the input string \"Indian\".\n",
    "# The run method formats the input string using each chain's prompt template in the order they are listed, passes the formatted prompts to the chains' language models, and returns the models' responses in the same order.\n",
    "# In this case, the input string \"Indian\" is likely used to replace the 'cuisine' placeholder in the first chain's prompt template and the 'restaurant_name' placeholder in the second chain's prompt template.\n",
    "# The resulting responses will be a suggestion for a restaurant name for Indian cuisine and some menu items for that restaurant, respectively.\n",
    "\n",
    "content = chain.invoke(\"Indian\")\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.7\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The output_key parameter is a string that specifies the key to be used for the output of the chain.\n",
    "# In this case, the output_key is \"restaurant_name\", which means the model's response will be stored under this key in the output.\n",
    "# The resulting LLMChain object is assigned to the variable 'name_chain'.\n",
    "# This 'name_chain' object can be used to generate predictions based on the specified model and prompt template, and store the results under the specified output key.\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "# Instantiate an OpenAI model with a temperature parameter of 0.7\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'restaurant_name' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"Suggest some menu items for {restaurant_name}.\"\n",
    "# So, when this template is used with a specific 'restaurant_name', it will generate a prompt for suggesting menu items for that restaurant.\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The output_key parameter is a string that specifies the key to be used for the output of the chain.\n",
    "# In this case, the output_key is \"menu_items\", which means the model's response will be stored under this key in the output.\n",
    "# The resulting LLMChain object is assigned to the variable 'food_items_chain'.\n",
    "# This 'food_items_chain' object can be used to generate predictions based on the specified model and prompt template, and store the results under the specified output key.\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': '\\n\\n\"Spice and Splendor: An Indian Culinary Experience\"',\n",
       " 'menu_items': '\\n\\n1. Appetizers:\\n- Samosas (crispy pastry filled with spiced potatoes and peas)\\n- Pakoras (deep-fried vegetable fritters)\\n- Aloo Tikki (potato patties with mint chutney)\\n- Tandoori Chicken Skewers\\n- Papdi Chaat (crispy flatbread topped with chickpeas, yogurt, and chutney)\\n- Masala Vada (spiced lentil fritters)\\n\\n2. Main dishes:\\n- Butter Chicken (tandoori chicken cooked in a creamy tomato-based sauce)\\n- Palak Paneer (spinach and cottage cheese curry)\\n- Biryani (fragrant basmati rice cooked with spices and choice of meat or vegetables)\\n- Dal Makhani (slow-cooked black lentils with cream and spices)\\n- Chana Masala (chickpea curry)\\n- Rogan Josh (spicy lamb curry)\\n\\n3. Side dishes:\\n- Naan (traditional Indian flatbread)\\n- Vegetable Pulao (fragrant basmati rice cooked with vegetables and spices)\\n- Raita (yogurt dip with cucumbers, tomatoes, and spices)\\n- Aloo Gobi (potato and cauliflower curry)\\n- Baing'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"cuisine\": \"Indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure yoy have installed this package: \n",
    "#pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"SERPAPI_API_KEY.txt\", \"r\") as f:\n",
    "  serpapi_key = ' '.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = serpapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Essential to use a temperature of 0.9; otherwise the tool function won't work\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We should use a calculator to perform this calculation.\n",
      "Action: Calculator\n",
      "Action Input: 2022 + 5\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2027\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The result is an estimated value for the GDP in 2027.\n",
      "Action: Search\n",
      "Action Input: GDP US 2027\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Its GDP is expected to expand by a little from $17.74 trillion in 2021 to $18.31 trillion in 2022 and to $26.43 trillion by 2027. 3. India. 2021 ...', \"Real GDP long-term forecastTotal, Million US dollars, 2020 – 2060 2020 ... 2027: 32 746 700 Million US dollars China (People's Republic of), 2028: 33 953 930 ...\", 'In the baseline scenario, the Goldman Sachs Research economists estimate AI could increase US productivity growth by 1.5 percentage points ...', 'GDP per capita, current prices. U.S. dollars per capita. 83.06. thousand. Created with Highcharts 6.1.4. 2024. United States ...', 'CBO expects real (inflation-adjusted) GDP to grow by 2.3 percent this year and by 1.9 percent next year. CBO. 2. Page 4. Projected Contributions to the ...', '2024 Projected Real GDP (% Change) : 2.1* ; Country Population: 336.692 million ; Date of Membership: December 27, 1945 ; Article IV/Country Report: June 15, 2023 ...', 'Discretionary spending drops from 6.3 percent of GDP in 2017 to 5.3 percent in 2027—a smaller percentage relative to the size of the economy ...', 'Real gross domestic product (GDP) increased at an annual rate of 3.3 percent in the fourth quarter of 2023, according to the \"advance\" estimate.']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is a lot of information about the GDP of the US in 2027.\n",
      "Action: Search\n",
      "Action Input: US GDP 2027\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m31,428.87\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m This is the projected GDP for the US in 2027.\n",
      "Final Answer: 31,428.87\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What was the GDP of US in 2022 plus 5?', 'output': '31,428.87'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it out!\n",
    "agent.invoke(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should gather information about Elon Musk's birth and current date and calculate his age.\n",
      "Action: Wikipedia\n",
      "Action Input: Elon musk\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor. He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect, and former chairman of Tesla, Inc.; owner, chairman, and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, and $210.2  billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.\n",
      "In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial intelligence company.\n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation and antisemitic conspiracy theories. His ownership of Twitter has been similarly controversial, being marked by the laying off of a large number of employees, an increase in hate speech and misinformation and disinformation on the website, as well as changes to Twitter Blue verification. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him, alleging that he had falsely announced that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\n",
      "\n",
      "Page: Twitter under Elon Musk\n",
      "Summary: Elon Musk completed his acquisition of Twitter in October 2022; Musk acted as CEO of Twitter until June 2023 when he was succeeded by Linda Yaccarino. Twitter was then rebranded to X in July 2023. Initially during Musk's tenure, Twitter introduced a series of reforms and management changes; the company reinstated a number of previously banned accounts, reduced the workforce by approximately 80%, closed one of Twitter's three data centers, and largely eliminated the content moderation team, replacing it with the crowd-sourced fact-checking system Community Notes.\n",
      "In November 2022, Twitter then began offering paid verification checkmarks, followed by removing legacy verification. In December, the Twitter Files were released and a number of journalists suspended from the platform. The foll\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now have information about Elon Musk's birth date and current date, I should calculate his age.\n",
      "Action: Calculator\n",
      "Action Input: February 2024 - June 1971\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 53\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Elon Musk will be 53 years old in February 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Elon Musk will be 53 years old in February 2024.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What is his age right now in February 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"El Dorado Cocina Mexicana\"\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Spice Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'memory' attribute of the 'chain' object.\n",
    "# The 'memory' attribute typically stores the state or history of the chain, \n",
    "# such as the inputs it has processed and the outputs it has generated.\n",
    "# This can be useful for debugging or for understanding how the chain has been operating.\n",
    "\n",
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"La Fiesta Mexicana\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sahara Palace\" or \"Aladdin's Oasis\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"La Fiesta Mexicana\"\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "\"Sahara Palace\" or \"Aladdin's Oasis\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was won by the West Indies in 1975.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is equal to 10.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the West Indies team that won the first cricket world cup was Clive Lloyd.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first cricket world cup was won by the West Indies in 1975.\n",
      "Human: How much is 5+5?\n",
      "AI:  5+5 is equal to 10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the West Indies team that won the first cricket world cup was Clive Lloyd.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first cricket world cup was won by the West Indies in 1975. They beat Australia in the final by 17 runs. The tournament was held in England and featured eight teams. Interestingly, the West Indies had a perfect record in the tournament, winning all their matches.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is equal to 10. This is a basic mathematical equation and can also be written as 5x2 or 10/1. Did you know that the concept of addition dates back to ancient civilizations such as the Egyptians and Babylonians? They used different methods, such as counting on fingers or using tally marks, to add numbers together.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I do not have enough information to answer that question. Can you provide me with the name of the team or the specific match you are referring to?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
