{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraies\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n",
    "folder_path = '/Users/jasper/Desktop/LangChain/'\n",
    "\n",
    "# Folder path\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
    "  openai.api_key = ' '.join(f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the OpenAI API key by updating the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI class\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.9\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Taj Mahal Bites\"\n",
      "2. \"Spice Route Kitchen\"\n",
      "3. \"Saffron Palace\"\n",
      "4. \"Curry Royal\"\n",
      "5. \"Naan Sensation\"\n",
      "6. \"Masala Mansion\"\n",
      "7. \"Nirvana Indian Cuisine\"\n",
      "8. \"Mughal Delight\"\n",
      "9. \"Samosa Garden\"\n",
      "10. \"Chutney Court\"\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "# response = llm.predict(\"What is the capital of the United States?\")\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of the United States is Washington, D.C. \n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "response = llm.predict(\"What is the capital of the United States?\")\n",
    "#name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Olive & Za\\'atar: A Taste of Palestine\"'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Palestinian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PromptTemplate class from the prompts module of the langchain package.\n",
    "# This class is  used to create, manipulate, or interact with prompts with the langchain framework.\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the format method of the PromptTemplate object to replace the placeholder in the template with a specific value.\n",
    "# In this case, the 'cuisine' placeholder in the template is replaced with the string \"Italian\".\n",
    "# The resulting formatted prompt is assigned to the variable 'p'.\n",
    "# So, 'p' will contain the string \"I want to open a restaurant for Italian food. Suggest a fancy name for this.\"\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "# print the prompt\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"La Bella Trattoria\" \n"
     ]
    }
   ],
   "source": [
    "# Use the predict method of the OpenAI model object (llm) to generate a prediction based on the input prompt 'p'.\n",
    "# The input prompt 'p' is a string that was formatted using a PromptTemplate object.\n",
    "# The predict method returns a string that is the model's response to the input prompt.\n",
    "# The print function is used to display this response in the console.\n",
    "\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLMChain class from the chains module of the langchain package.\n",
    "# This class is used to create, manipulate, or interact with language model chains in the context of the langchain framework.\n",
    "\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The resulting LLMChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Mexican', 'text': '\\n\\n\"Hacienda del Sabor\" '}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the invoke method of the LLMChain object to generate a prediction based on the input string \"Mexican\".\n",
    "# The invoke method formats the input string using the chain's prompt template, passes the formatted prompt to the chain's language model, and returns the model's response.\n",
    "# In this case, the input string \"Mexican\" is likely used to replace the 'cuisine' placeholder in the prompt template.\n",
    "# So, the model's response will be a suggestion for a restaurant name for Mexican cuisine.\n",
    "\n",
    "#chain.run(\"Mexican\") -> run command is depreciated in the new version of langchain\n",
    "chain.invoke(\"Mexican\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Entering new  chain...\n",
    "Prompt after formatting:\n",
    "I want to open a restaurant for Mexican food. Suggest a fency name for this.\n",
    "\n",
    "> Finished chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.6\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with the specified OpenAI model and prompt template.\n",
    "# The resulting LLMChain object is assigned to the variable 'name_chain'.\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'restaurant_name' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"Suggest some menu items for {restaurant_name}\"\n",
    "# So, when this template is used with a specific 'restaurant_name', it will generate a prompt for suggesting menu items for that restaurant.\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# In this case, the prompt template is designed to generate prompts for suggesting menu items for a specific restaurant.\n",
    "# The resulting LLMChain object is assigned to the variable 'food_items_chain'.\n",
    "# This 'food_items_chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SimpleSequentialChain class from the chains module of the langchain package.\n",
    "# This class is likely used to create, manipulate, or interact with simple sequential chains in the context of the langchain package.\n",
    "# A sequential chain is a type of model that processes input data in a sequential manner, which is often used in natural language processing.\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a SimpleSequentialChain object with specified parameters.\n",
    "# The chains parameter is a list of LLMChain objects that will be used by the SimpleSequentialChain in the order they are listed.\n",
    "# In this case, the list contains 'name_chain' and 'food_items_chain', which means 'name_chain' will be run first, followed by 'food_items_chain'.\n",
    "# The resulting SimpleSequentialChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified sequence of chains.\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Indian', 'output': '\\n\\n1. Butter Chicken: A classic Indian dish made with tender chicken in a creamy tomato-based sauce infused with aromatic spices.\\n\\n2. Tandoori Naan: Soft and fluffy flatbread cooked in a traditional clay oven, perfect for scooping up the delicious curries.\\n\\n3. Vegetable Samosas: Crispy pastry filled with a savory mixture of potatoes, peas, and spices.\\n\\n4. Lamb Rogan Josh: A hearty curry made with tender chunks of lamb in a rich and flavorful sauce.\\n\\n5. Aloo Gobi: A vegetarian dish made with potatoes and cauliflower cooked in a fragrant blend of spices.\\n\\n6. Chicken Biryani: A fragrant rice dish made with tender chicken, aromatic spices, and saffron.\\n\\n7. Palak Paneer: Creamy spinach curry with cubes of soft paneer cheese.\\n\\n8. Chana Masala: A popular North Indian dish made with chickpeas cooked in a flavorful blend of spices.\\n\\n9. Mango Lassi: A refreshing yogurt-based drink flavored with sweet mangoes and a hint of cardamom.\\n\\n10. Gulab Jamun: Soft and spongy milk-based balls soaked in a sweet syrup, a perfect way to end the meal.'}\n"
     ]
    }
   ],
   "source": [
    "# Use the run method of the SimpleSequentialChain object to generate a prediction based on the input string \"Indian\".\n",
    "# The run method formats the input string using each chain's prompt template in the order they are listed, passes the formatted prompts to the chains' language models, and returns the models' responses in the same order.\n",
    "# In this case, the input string \"Indian\" is likely used to replace the 'cuisine' placeholder in the first chain's prompt template and the 'restaurant_name' placeholder in the second chain's prompt template.\n",
    "# The resulting responses will be a suggestion for a restaurant name for Indian cuisine and some menu items for that restaurant, respectively.\n",
    "\n",
    "content = chain.invoke(\"Indian\")\n",
    "\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.7\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The output_key parameter is a string that specifies the key to be used for the output of the chain.\n",
    "# In this case, the output_key is \"restaurant_name\", which means the model's response will be stored under this key in the output.\n",
    "# The resulting LLMChain object is assigned to the variable 'name_chain'.\n",
    "# This 'name_chain' object can be used to generate predictions based on the specified model and prompt template, and store the results under the specified output key.\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.7\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'restaurant_name' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"Suggest some menu items for {restaurant_name}.\"\n",
    "# So, when this template is used with a specific 'restaurant_name', it will generate a prompt for suggesting menu items for that restaurant.\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The output_key parameter is a string that specifies the key to be used for the output of the chain.\n",
    "# In this case, the output_key is \"menu_items\", which means the model's response will be stored under this key in the output.\n",
    "# The resulting LLMChain object is assigned to the variable 'food_items_chain'.\n",
    "# This 'food_items_chain' object can be used to generate predictions based on the specified model and prompt template, and store the results under the specified output key.\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'restaurant_name': '\\n\\n\"Spice Lounge\"',\n",
       " 'menu_items': '\\n\\n1. Tandoori Chicken Skewers\\n2. Lamb Vindaloo\\n3. Vegetable Samosas\\n4. Butter Chicken\\n5. Garlic Naan Bread\\n6. Saag Paneer\\n7. Chicken Tikka Masala\\n8. Vegetable Biryani\\n9. Aloo Gobi (potatoes and cauliflower)\\n10. Mango Lassi (yogurt drink)\\n11. Chana Masala (chickpea curry)\\n12. Papri Chaat (crispy snack with yogurt and chutney)\\n13. Chicken Korma\\n14. Dal Makhani (creamy lentil dish)\\n15. Gulab Jamun (dessert of fried dough balls in syrup)'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"cuisine\": \"Indian\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure yoy have installed this package: \n",
    "#pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"SERPAPI_API_KEY.txt\", \"r\") as f:\n",
    "  serpapi_key = ' '.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = serpapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## serpapi and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential to use a temperature of 0.9; otherwise the tool function won't work\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use a calculator to solve this math problem.\n",
      "Action: Calculator\n",
      "Action Input: 2022 + 5\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2027\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I need to search for the GDP of the US in 2022.\n",
      "Action: Search\n",
      "Action Input: \"GDP of US 2022\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m$25.46 trillion\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The GDP of the US in 2022 plus 5 is $25.46 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What was the GDP of US in 2022 plus 5?',\n",
       " 'output': 'The GDP of the US in 2022 plus 5 is $25.46 trillion.'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it out!\n",
    "agent.invoke(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia and llm-math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m First, I need to find out the birthdate of Elon Musk. Then, I need to calculate his age based on the given date in February 2024.\n",
      "Action: Wikipedia\n",
      "Action Input: Elon Musk\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Elon Musk\n",
      "Summary: Elon Reeve Musk (; EE-lon; born June 28, 1971) is a businessman and investor. He is the founder, chairman, CEO, and CTO of SpaceX; angel investor, CEO, product architect, and former chairman of Tesla, Inc.; owner, chairman, and CTO of X Corp.; founder of the Boring Company and xAI; co-founder of Neuralink and OpenAI; and president of the Musk Foundation. He is the wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, and $210.2  billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.A member of the wealthy South African Musk family, Elon was born in Pretoria and briefly attended the University of Pretoria before immigrating to Canada at age 18, acquiring citizenship through his Canadian-born mother. Two years later, he matriculated at Queen's University at Kingston in Canada. Musk later transferred to the University of Pennsylvania, and received bachelor's degrees in economics and physics. He moved to California in 1995 to attend Stanford University. However, Musk dropped out after two days and, with his brother Kimbal, co-founded online city guide software company Zip2. The startup was acquired by Compaq for $307 million in 1999, and, that same year Musk co-founded X.com, a direct bank. X.com merged with Confinity in 2000 to form PayPal.\n",
      "In October 2002, eBay acquired PayPal for $1.5 billion, and that same year, with $100 million of the money he made, Musk founded SpaceX, a spaceflight services company. In 2004, he became an early investor in electric vehicle manufacturer Tesla Motors, Inc. (now Tesla, Inc.). He became its chairman and product architect, assuming the position of CEO in 2008. In 2006, Musk helped create SolarCity, a solar-energy company that was acquired by Tesla in 2016 and became Tesla Energy. In 2013, he proposed a hyperloop high-speed vactrain transportation system. In 2015, he co-founded OpenAI, a nonprofit artificial intelligence research company. The following year, Musk co-founded Neuralink—a neurotechnology company developing brain–computer interfaces—and the Boring Company, a tunnel construction company. In 2022, he acquired Twitter for $44 billion. He subsequently merged the company into newly created X Corp. and rebranded the service as X the following year. In March 2023, he founded xAI, an artificial intelligence company.\n",
      "Musk has expressed views that have made him a polarizing figure. He has been criticized for making unscientific and misleading statements, including COVID-19 misinformation and antisemitic conspiracy theories. His ownership of Twitter has been similarly controversial, being marked by the laying off of a large number of employees, an increase in hate speech and misinformation and disinformation on the website, as well as changes to Twitter Blue verification. In 2018, the U.S. Securities and Exchange Commission (SEC) sued him, alleging that he had falsely announced that he had secured funding for a private takeover of Tesla. To settle the case, Musk stepped down as the chairman of Tesla and paid a $20 million fine.\n",
      "\n",
      "Page: Twitter under Elon Musk\n",
      "Summary: Elon Musk completed his acquisition of Twitter in October 2022; Musk acted as CEO of Twitter until June 2023 when he was succeeded by Linda Yaccarino. Twitter was then rebranded to X in July 2023. Initially during Musk's tenure, Twitter introduced a series of reforms and management changes; the company reinstated a number of previously banned accounts, reduced the workforce by approximately 80%, closed one of Twitter's three data centers, and largely eliminated the content moderation team, replacing it with the crowd-sourced fact-checking system Community Notes.\n",
      "In November 2022, Twitter then began offering paid verification checkmarks, followed by removing legacy verification. In December, the Twitter Files were released and a number of journalists suspended from the platform. The foll\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now that I have the birthdate of Elon Musk, I can calculate his age in February 2024.\n",
      "Action: Calculator\n",
      "Action Input: 6/28/1971, 2/1/2024\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 0.001096861579291597\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 52 years old\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'52 years old'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"When was Elon musk born? What is his age right now in February 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Casa Mexicana\" \n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Spice Haven\" \n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'memory' attribute of the 'chain' object.\n",
    "# The 'memory' attribute typically stores the state or history of the chain, \n",
    "# such as the inputs it has processed and the outputs it has generated.\n",
    "# This can be useful for debugging or for understanding how the chain has been operating.\n",
    "\n",
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"La Fiesta Mexicana\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Arabian Delights\" or \"Taste of the East\" \n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Arabic\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\n",
      "\"La Fiesta Mexicana\"\n",
      "Human: Arabic\n",
      "AI: \n",
      "\n",
      "\"Arabian Delights\" or \"Taste of the East\" \n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The first cricket world cup was held in 1975 and was won by the West Indies. The tournament was organized by the International Cricket Council and featured eight teams from around the world. The final match was played between the West Indies and Australia at Lord's Cricket Ground in London. The West Indies won by 17 runs, with Clive Lloyd scoring 102 runs and Joel Garner taking 5 wickets.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is equal to 10.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team, the West Indies, was Clive Lloyd. He was a left-handed batsman and occasional medium-pace bowler. He was known for his aggressive and powerful batting style and is considered one of the greatest cricketers in history. He led the West Indies team to two consecutive world cup victories in 1975 and 1979.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain ofthe winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who won the first cricket world cup?\n",
      "AI:  The first cricket world cup was held in 1975 and was won by the West Indies. The tournament was organized by the International Cricket Council and featured eight teams from around the world. The final match was played between the West Indies and Australia at Lord's Cricket Ground in London. The West Indies won by 17 runs, with Clive Lloyd scoring 102 runs and Joel Garner taking 5 wickets.\n",
      "Human: How much is 5+5?\n",
      "AI:  5+5 is equal to 10.\n",
      "Human: Who was the captain ofthe winning team?\n",
      "AI:  The captain of the winning team, the West Indies, was Clive Lloyd. He was a left-handed batsman and occasional medium-pace bowler. He was known for his aggressive and powerful batting style and is considered one of the greatest cricketers in history. He led the West Indies team to two consecutive world cup victories in 1975 and 1979.\n"
     ]
    }
   ],
   "source": [
    "print(convo.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
