{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraies\n",
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n",
    "folder_path = '/Users/jasper/Desktop/LangChain/'\n",
    "\n",
    "# Folder path\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
    "  openai.api_key = ' '.join(f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the OpenAI API key by updating the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OpenAI class\n",
    "\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an OpenAI model with a temperature parameter of 0.9\n",
    "# The temperature parameter controls the randomness of the model's output.\n",
    "# A higher temperature value (closer to 1) results in more random output,\n",
    "# while a lower value (closer to 0) results in more deterministic output.\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Maharaja's Palace\" or \"Spice Kingdom\"\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "# response = llm.predict(\"What is the capital of the United States?\")\n",
    "name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of the United States is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "response = llm.predict(\"What is the capital of the United States?\")\n",
    "#name = llm.predict(\"I want to open a restaurant for Indian food. Suggest a fency name for this.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Sahafti Al-Filasteeni\" (My Palestinian Kitchen)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"I want to open a restaurant for Palestinian food. Suggest a fency name for this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PromptTemplate class from the prompts module of the langchain package.\n",
    "# This class is  used to create, manipulate, or interact with prompts with the langchain framework.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PromptTemplate object with specified input variables and template.\n",
    "# The input_variables parameter is a list of variable names that will be used in the template.\n",
    "# In this case, 'cuisine' is the only input variable.\n",
    "# The template parameter is a string that contains placeholders for the input variables.\n",
    "# These placeholders are denoted by the variable names enclosed in curly braces {}.\n",
    "# In this case, the template is \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "# So, when this template is used with a specific 'cuisine', it will generate a prompt for suggesting a restaurant name for that cuisine.\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the format method of the PromptTemplate object to replace the placeholder in the template with a specific value.\n",
    "# In this case, the 'cuisine' placeholder in the template is replaced with the string \"Italian\".\n",
    "# The resulting formatted prompt is assigned to the variable 'p'.\n",
    "# So, 'p' will contain the string \"I want to open a restaurant for Italian food. Suggest a fancy name for this.\"\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Italian food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "# print the prompt\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Casa di Gastronomia\"\n"
     ]
    }
   ],
   "source": [
    "# Use the predict method of the OpenAI model object (llm) to generate a prediction based on the input prompt 'p'.\n",
    "# The input prompt 'p' is a string that was formatted using a PromptTemplate object.\n",
    "# The predict method returns a string that is the model's response to the input prompt.\n",
    "# The print function is used to display this response in the console.\n",
    "\n",
    "print(llm.predict(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LLMChain class from the chains module of the langchain package.\n",
    "# This class is used to create, manipulate, or interact with language model chains in the context of the langchain framework.\n",
    "\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an LLMChain object with specified parameters.\n",
    "# The llm parameter is an instance of an OpenAI model, which will be used by the LLMChain for generating predictions.\n",
    "# The prompt parameter is an instance of a PromptTemplate, which will be used by the LLMChain to format input prompts.\n",
    "# The resulting LLMChain object is assigned to the variable 'chain'.\n",
    "# This 'chain' object can be used to generate predictions based on the specified model and prompt template.\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Mexican', 'text': '\\n\"El Sabroso Restaurante\" '}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the invoke method of the LLMChain object to generate a prediction based on the input string \"Mexican\".\n",
    "# The invoke method formats the input string using the chain's prompt template, passes the formatted prompt to the chain's language model, and returns the model's response.\n",
    "# In this case, the input string \"Mexican\" is likely used to replace the 'cuisine' placeholder in the prompt template.\n",
    "# So, the model's response will be a suggestion for a restaurant name for Mexican cuisine.\n",
    "\n",
    "#chain.run(\"Mexican\") -> run command is depreciated in the new version of langchain\n",
    "chain.invoke(\"Mexican\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
