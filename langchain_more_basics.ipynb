{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and modules\n",
    "import langchain\n",
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the API key by reading the folder path. Use this code if you're running the code on Google Colab. Otherwise, use the actual folder path\n",
    "folder_path = '/Users/jasper/Desktop/LangChain/'\n",
    "\n",
    "# Folder path\n",
    "os.chdir(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file containing the API key\n",
    "with open(folder_path + \"Jasper_OpenAI_API_Key.txt\", \"r\") as f:\n",
    "  openai.api_key = ' '.join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the OpenAI API key by updating the environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will provided with the sample text. Your task is to rewrite the text to be gramatically correct. Sample text: ```Me likes cats not dogs. They jumps high so much!``` Output: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create a string template with `sample_text` input variable\n",
    "template = \"\"\"You will provided with the sample text. \\\n",
    "Your task is to rewrite the text to be gramatically correct. \\\n",
    "Sample text: ```{sample_text}``` \\\n",
    "Output: \n",
    "\"\"\"\n",
    "# create a prompt template using above-defined template string \n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "# specify the `sample_text` variable\n",
    "sample_text = \"Me likes cats not dogs. They jumps high so much!\"\n",
    "# generate a final prompt by passing `sample_text` variable\n",
    "final_prompt = prompt_template.format(\n",
    "    sample_text=sample_text\n",
    ")\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You will provided with the sample text. Your task is to translate the text into English language and summarize the translated text in at most 15 words. \\\\ \\n'), HumanMessage(content='Estoy deseando que llegue el fin de semana.')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# create a string template for a System role with two input variable: `output_language` and `max_words`\n",
    "system_template = \"\"\"You will provided with the sample text. \\\n",
    "Your task is to translate the text into {output_language} language \\\n",
    "and summarize the translated text in at most {max_words} words. \\ \n",
    "\"\"\"\n",
    "# create a prompt template for a System role\n",
    "system_message_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    system_template)\n",
    "\n",
    "# create a string template for a System role with `sample_text` input variable\n",
    "human_template = \"{sample_text}\"\n",
    "\n",
    "# create a prompt template for a Human role\n",
    "human_message_prompt_template = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# create chat prompt template out of one or several message prompt templates\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt_template, human_message_prompt_template])\n",
    "\n",
    "# generate a final prompt by passing all three variables (`output_language`,  `max_words`, `sample_text`)\n",
    "final_prompt = chat_prompt_template.format_prompt(output_language=\"English\", max_words=15,\n",
    "                          sample_text=\"Estoy deseando que llegue el fin de semana.\").to_messages()\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_community/llms/openai.py:1061: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:115: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like cats, not dogs. They jump so high!\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# initialize GPT-3.5 model, remember that temperature parameter defines randomness of the response\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "template = \"\"\"You will provided with the sample text. \\\n",
    "Your task is to rewrite the text to be gramatically correct. \\\n",
    "Sample text: ```{sample_text}``` \\\n",
    "Output: \n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "sample_text = \"Me likes cats not dogs. They jumps high so much!\"\n",
    "final_prompt = prompt_template.format(\n",
    "    sample_text=sample_text\n",
    ")\n",
    "# generate the output by calling GPT model and passing the prompt\n",
    "completion = llm(final_prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
